{
  "cells": [
    {
      "metadata": {
        "_uuid": "fdf7fa7428f44183c55423fa00cae00c93460470"
      },
      "cell_type": "markdown",
      "source": "# NYPD Complaint Data Cleaning for Beginners\n### Data updated 7 June 2018\n\n### Notebook created - *August 2018*\n## All updates written out at the end.\n### Version - 2"
    },
    {
      "metadata": {
        "_uuid": "5e0abd03a42166434a7662720f938a59e5d6724f"
      },
      "cell_type": "markdown",
      "source": "## Introduction\n\n\nThis notebook was made as a product of data cleaning and dealing with NaNs skills practise. Many NaN values from this kernel could be eliminated in other, simpler ways without dataset damage. This is my first kernel and I'm a novice in data science, so if you find any error or you know a better solution to get same or similar results, please post in comment. What is more, I do my best with my english language writting skills, so please be understanding."
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "collapsed": true,
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": false
      },
      "cell_type": "markdown",
      "source": "## Activities I am planning to perform in this kernel\n\n\n### [Data exploration and cleaning:](#1)\n1. [First things first](#2)\n2. [Import data and explore column data types](#3)\n3. [Drop values/columns](#4)\n4. [Fill NaNs](#5)\n5. [Which borough?](#6)\n6. [Final toughts](#7)"
    },
    {
      "metadata": {
        "_uuid": "fdb46bc712cb353fc827dc20d60211c4a8aa90c8"
      },
      "cell_type": "markdown",
      "source": "<a id=\"1\"></a> <br>\n## DATA EXPLORATION AND CLEANING"
    },
    {
      "metadata": {
        "_uuid": "239adfea816eef90b4ae763f136a3684809c61cc"
      },
      "cell_type": "markdown",
      "source": "<a id=\"2\"></a> <br>\n### FIRST THINGS FIRST"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f6bce90d60a848c675379e3e787777334a239093"
      },
      "cell_type": "code",
      "source": "# Imports\n\n# Visualisations\nimport matplotlib.pyplot as plt \nimport matplotlib\nimport plotly.plotly as py\nimport plotly.graph_objs as go\nimport seaborn as sns\nplt.style.use('fivethirtyeight')\n%matplotlib inline\n\n# Warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Data exploration\nimport pandas as pd\n\n# Numerical\nimport numpy as np\n\n# Spatial analysis\nimport geopandas as gpd   # used to read .shp file\nfrom shapely.geometry import Point, Polygon, shape\nimport utm   # Need to be installed, coordinates conversion\n\n# Regular expressions\nimport re\n\n# Random\nnp.random.seed(11)\n\n# Files in dataset\nimport os\nprint(os.listdir(\"../input\"))\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ca57f5893f504bef6adfc444a279f0975b81a180"
      },
      "cell_type": "code",
      "source": "# Import data\ncrimes_raw = pd.read_csv('../input/nypd-complaint-data-current-ytd-july-2018/NYPD_Complaint_Data_Current_YTD.csv')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "6018e092fd80a51ee7a1497f38dfa0cbc091187c"
      },
      "cell_type": "markdown",
      "source": "<a id=\"3\"></a> <br>\n### IMPORT DATA AND EXPLORE COLUMN DATA TYPES"
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "_uuid": "90ba51f07637d0f0a11c0c3d677d2480733898fa"
      },
      "cell_type": "code",
      "source": "# Columns data info\ncrimes_raw.info()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "50271fd0e71c0547c59c012a819aa8ec2879c900",
        "scrolled": true
      },
      "cell_type": "code",
      "source": "crimes_raw.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "87e3e41131dc349911a8442a903b679c93910a6f"
      },
      "cell_type": "markdown",
      "source": "<a id=\"4\"></a> <br>\n### DROP VALUES/COLUMNS"
    },
    {
      "metadata": {
        "_uuid": "aac8dc7dd179167588039c1335eee9462aa9bc35"
      },
      "cell_type": "markdown",
      "source": "<br>As a begin, drop columns that contain duplicated or useless information for this kernel. I decided to drop *HADEVELOPT* ,  *HOUSING_PSA*,  *LOC_OF_OCCUR_DESC*,  *PD_CD* columns.\n<br>\n<br>***HADEVELOPT*** - too detailed \n<br>***HOUSING_PSA*** - useless\n<br>***LOC_OF_OCCUR_DESC*** - useless\n<br>***PD_CD*** - too detailed, duplicated\n\n<br> *X_COORD_CD* and *Y_COORD_CD* columns contain the similar information about location as columns *Latitude* and *Longitude* (other reference systems). In some cases I will use  *X_COORD_CD* and *Y_COORD_CD* columns' values in other *Latitude* and *Longitude*. No need for unit conversion if all values are currently given. Lat and lon coordinates are more intuitive and conveninet for me, so I'll use them."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5109f312fed0b541eb1f42d0487b456bf07133db"
      },
      "cell_type": "code",
      "source": "# Columns to drop\ncrimes = crimes_raw.drop(['HADEVELOPT', 'HOUSING_PSA', 'LOC_OF_OCCUR_DESC', 'PD_CD'], axis = 1)\n\n# Drop additional columns\n# crimes = crimes.drop(['X_COORD_CD', 'Y_COORD_CD', 'Lat_Lon'], axis = 1)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "c1cc94c3e4b35d96432a25b750a2b6ad6b2f0611"
      },
      "cell_type": "markdown",
      "source": "<a id=\"5\"></a> <br>\n### FILL NANS"
    },
    {
      "metadata": {
        "_uuid": "ad91138b6e762f5726511712c65edac29a7d72cc"
      },
      "cell_type": "markdown",
      "source": "<br>Quite a few columns have missing values. There are several types of missing values: \n<br>**NaN**  -  no value\n<br>**UNKNOWN** - in some cases it means value is unknown, in other it is label as unknow to prevent identifying victims\n<br>**outliers** - values inadequate for given category (e.g. *SUSP_AGE_GROUP*, value: *978*)\n<br>Almost all missing values will be filled. Several columns with incomplete data might have been dropped, but have been preserved in order to practice filling NaNs.\n<br> Details of dealing with NaNs divided into columns are below."
    },
    {
      "metadata": {
        "_uuid": "348e47211260e50194d9848d18caf8eb0e00d72f"
      },
      "cell_type": "markdown",
      "source": "<br>***CMPLN_TO_DT*** - filled missing values with *CMPLNT_FR_DT* means incident occured during one day\n<br>***CMPLNT_TO_TM*** - filled missing values with *CMPLMNT_FR_TM* means incident occured but did no last - it is a kind of simplification, detailed analysis of incidents' time is not point of thie kernel"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "29d15db5f111e3141a347077f25ba5c8ce559dab"
      },
      "cell_type": "code",
      "source": "# Fill CMPLNT_TO_DT NaNs with CMPLNT_FR_DT values.\ncrimes['CMPLNT_TO_DT'].fillna(crimes['CMPLNT_FR_DT'], axis = 0, inplace = True)\n\n# Fill CMPLNT_TO_TM NaNs with CMPLNT_FR_TM values.\ncrimes['CMPLNT_TO_TM'].fillna(crimes['CMPLNT_FR_TM'], axis = 0, inplace = True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "7826ce9264f876a245955dfc14e9f74e90f211be"
      },
      "cell_type": "markdown",
      "source": "<br>***JURISDICTION_CODE*** - matched *JURISDICTION_CODE* values to *JURIS_DESC* and filled *JURISDICTION_CODE* NaNs"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "521f5ffdd82be7715f407ea30c815d32dcb3def9",
        "scrolled": true
      },
      "cell_type": "code",
      "source": "# Find jurisdiction codes for all jurisdictions responsible for incident\njuris_desc = crimes[['JURISDICTION_CODE', 'JURIS_DESC']].drop_duplicates(subset = 'JURIS_DESC', keep = 'first').reset_index(drop = True)\njuris_desc.rename(columns = {'JURIS_DESC' : 'JUR_NAME'}, inplace = True) #change column name to simplify distinction of colmumns` names\nprint(juris_desc)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2f2b95d2d928e5cf2395267c7f12c0f84f98abda"
      },
      "cell_type": "code",
      "source": "# Fill NaN values with some number (different from any JURISDICTION_CODE).\ncrimes['JURISDICTION_CODE'].fillna(5, inplace = True)\n\n# Replacement of filled NaNs values with appropriate numbers.\n# I don`t understand how below line exactly works, but I found similar solution on the web (and can`t find source again to analyze).\ncrimes['JURISDICTION_CODE'] = crimes['JURISDICTION_CODE'] \\\n                              .mask(crimes['JURISDICTION_CODE'] == 5, crimes['JURIS_DESC'] \\\n                              .map(juris_desc.set_index('JUR_NAME')['JURISDICTION_CODE']))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "8d15168a65c0c71b8686e9e049c9e401cb1a834c"
      },
      "cell_type": "markdown",
      "source": "<br>***OFNS_DESC*** -  'OFNS_DESC' column has only 4 NaN values. Rows with those values have values in PD_DESC, so the best solution is to copy them.\n<br>***PD_DESC*** - NaN values filled with *OFNS_DESC* values"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9240d65edb6910938d0c00cd9e2667c489238fcc"
      },
      "cell_type": "code",
      "source": "# All NaNs from 'PD_DESC' series are filled with copy of 'OFNS_DESC' values\ncrimes['OFNS_DESC'] = np.where(crimes['OFNS_DESC'].isnull(), crimes['PD_DESC'], crimes['OFNS_DESC']) # There is pandas equivalent of np.where -> https://stackoverflow.com/questions/38579532/pandas-equivalent-of-np-where\n# And vice versa\ncrimes['PD_DESC'] = np.where(crimes['PD_DESC'].isnull(), crimes['OFNS_DESC'], crimes['PD_DESC'])\n\n# Sanity check\nprint(crimes['OFNS_DESC'].notnull().value_counts()) \nprint(crimes['PD_DESC'].notnull().value_counts())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "6a98b3594ee822a36b8653e575a89e3d7b59117f"
      },
      "cell_type": "markdown",
      "source": "<br>***PRAKS_NM*** - NANs filled with 'NON APPLICABLE' statement\n<br>***STATION_NAME*** - NANs filled with 'NON APPLICABLE' statement\n<br>***TRANSIT_DISTRICT***  - NaNs filled with 0, all present values are numbers"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0d0f2e183971734ace4ad6fb1c17076ff23cda73"
      },
      "cell_type": "code",
      "source": "# Fill NaNs in 'PARKS_NM', 'STATION_NAME' series\ncrimes['PARKS_NM'].fillna('NON APPLICABLE', inplace = True)\ncrimes['STATION_NAME'].fillna('NON APPLICABLE', inplace = True)\n\n#Fill NaNs in 'TRANSIT_DISTRICT' with zeros, all different values are numbers, so it is good solution\ncrimes['TRANSIT_DISTRICT'].fillna(0, inplace = True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "0951dc643bc2274c274ed79159e9825595de3a3b"
      },
      "cell_type": "markdown",
      "source": "<br>***Latitude*** & ***Longitude*** - there are 2 rows with NaNs in these columns, so I chceked *ADDR_PCT_CD* value and found that is Bronx, picked random location in Bronx (manually) for these 2 NaNs\n<br> ***X_COORD_CD*** & ***Y_COORD_CD*** - NaNs filled with converted *Latitude* and *Longitude*\n<br> ***Lat_Lon*** - filled with latitude and longitude as a string type"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2f38b3ae8ffc5ff391eb14f60132f14aaedadc10"
      },
      "cell_type": "code",
      "source": "# There are 2 rows without values in localization series, both are from BRONX, fill it with location from bronx according to ADDR_PCT_CD value\ncrimes['Latitude'].fillna(40.821054, inplace = True)\ncrimes['Longitude'].fillna(-73.893848, inplace = True)\n\n# Convert coordinates from latitude, longitide to UTM - 2 BRONX values\nconverted = utm.from_latlon(crimes.iloc[2571,28], crimes.iloc[2571,29])\nconverted = list(converted)\ncrimes['X_COORD_CD'].fillna(converted[0], inplace = True)\ncrimes['Y_COORD_CD'].fillna(converted[1], inplace = True)\n\n\n# Fill Lat_Lon series values\nlat_lon = '(' + crimes['Latitude'].astype(str) + ', ' + crimes['Longitude'].astype(str) + ')'   # It`s important to apply \"(...).astype(str)\" not \"str(...)\" below - I made this mistake\ncrimes['Lat_Lon'].fillna(value = lat_lon, axis = 0, inplace = True)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "c41377ed80c1a1c8c0e805a8e8fefc243c20d855"
      },
      "cell_type": "markdown",
      "source": "<br>***PREM_TYP_DESC*** - fill NaNs with randomly picked values from locations with appropriate content"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "89856f0057ea1dec99a3ef7c14ce701ae24c7d38"
      },
      "cell_type": "code",
      "source": "# Creat copy to calculate proportions.\nprem_typ_desc_copy = crimes['PREM_TYP_DESC'].copy(deep = True)\nprem_typ_desc_copy_rand = prem_typ_desc_copy.value_counts(normalize = True).sort_values(ascending = False)\n\n# Fill PREM_TYP_DESC values NaN values with values from locations of other incidents.\ncrimes['PREM_TYP_DESC'] = crimes['PREM_TYP_DESC'].apply(lambda x: np.random.choice([x for x in crimes.prem_typ_desc],\n                          replace = True, p = prem_typ_desc_copy_rand ) if (x == np.nan) else x).astype(str)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "d4a23053e3bed32362fa9631b1fa93b9e8480470"
      },
      "cell_type": "markdown",
      "source": "<br> Excluding ***BORO_NM*** three series with NaN values left: ***SUSP_AGE_GROUP***, ***SUSP_RACE***, ***SUSP_SEX***. Easier way to fill them is to find distributions of race, age_group and gender proportions, but for now I leave it with NaNs and add more 3 columns with filled NaNs. I plan to do it in two ways. First, like previous column I will apply random function. Second way is to try implement machine learnig to estimate age_group, race and gender of suspector. Finally I'll make a comparison of values obtained in two ways. Directly below is version with ranadomly distributed age_group,  sex and race. ***PATROL_BORO*** column also has NaNs. I'll try implement clustring algorith to fill values in next kernel. "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "af59c3cd01f8f29830da26260651ed1a3b52a0fc"
      },
      "cell_type": "code",
      "source": "# Check values. \nprint(crimes['SUSP_AGE_GROUP'].value_counts())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "02648ee76f77913cf5a7e9918071c76d5f68509c"
      },
      "cell_type": "code",
      "source": "# Ascribe new age group for all suspectors except those which age group is 25-44, 18-24, 45-64, <18, or 65+ and do it in a new column\n# Create column for a new variable: suspector_age_rand and fill it wit 0s\ncrimes['suspector_age_rand'] = pd.Series(len(crimes['SUSP_AGE_GROUP']), index = crimes.index)\ncrimes['suspector_age_rand'] = 0   # Fill with 0.\n\n# Randomly stick age gruop to every user with NaN value\ncrimes.loc[(crimes['SUSP_AGE_GROUP'] != '25-44') | \n           (crimes['SUSP_AGE_GROUP'] != '18-24') |\n           (crimes['SUSP_AGE_GROUP'] != '45-64') |\n           (crimes['SUSP_AGE_GROUP'] != '65+') |\n           (crimes['SUSP_AGE_GROUP'] != '<18') |\n           (crimes['SUSP_AGE_GROUP'].isnull()), 'suspector_age_rand'] = np.nan\ncrimes.loc[(crimes['SUSP_AGE_GROUP'] == '25-44') | \n           (crimes['SUSP_AGE_GROUP'] == '18-24') |\n           (crimes['SUSP_AGE_GROUP'] == '45-64') |\n           (crimes['SUSP_AGE_GROUP'] == '65+') |\n           (crimes['SUSP_AGE_GROUP'] == '<18'), 'suspector_age_rand'] = crimes['SUSP_AGE_GROUP']\n\n\n# Create copy to calculate proportions\nsuspector_age_rand_copy = crimes['suspector_age_rand'].copy(deep = True)\n\n# Fill NaN values. It wouldn`t work with NaN values, so I replaced it\ncrimes['suspector_age_rand'].fillna(value = 1, inplace = True)\n\n# Obtain values for every age group\nsuspector_age_rand_copy.dropna(axis = 0, inplace = True)\nsorted_suspector_age_rand = suspector_age_rand_copy.value_counts(normalize = True).sort_index()\n\n# Fill NaNs (rightly ones) values in suspector_age_rand with randomly picked from random choice\ncrimes['suspector_age_rand'] = crimes['suspector_age_rand'].apply(lambda x: np.random.choice([x for x in sorted_suspector_age_rand.index],\n                               replace = True, p = sorted_suspector_age_rand) if (x == 1) else x).astype(str)\nprint(\"Suspector age with filled NaNs normalized:\\n\", crimes['suspector_age_rand'].value_counts(normalize = True))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e7c94c3be6c79250e87f67a3bb8c6727282e0122"
      },
      "cell_type": "code",
      "source": "# Similar operations for suspector race.\n# I wonder about writing a function here, I type similar code 2nd time...\nprint(\"Original data:\\n\",crimes['SUSP_RACE'].value_counts())\n\n# Create column for new variable suspector_age.\ncrimes['suspector_race_rand'] = pd.Series(len(crimes['SUSP_RACE']), index=crimes.index)\ncrimes['suspector_race_rand'] = 0\n\n# Randomly stick age gruop to every user with NaN value \ncrimes.loc[(crimes['SUSP_RACE'] != 'BLACK') | \n           (crimes['SUSP_RACE'] != 'WHITE HISPANIC') |\n           (crimes['SUSP_RACE'] != 'WHITE') |\n           (crimes['SUSP_RACE'] != 'BLACK HISPANIC') |\n           (crimes['SUSP_RACE'] != 'ASIAN/PAC.ISL') |\n           (crimes['SUSP_RACE'] != 'AMER IND') |\n           (crimes['SUSP_RACE'].isnull() == True), 'suspector_race_rand'] = np.nan\ncrimes.loc[(crimes['SUSP_RACE'] == 'BLACK') | \n           (crimes['SUSP_RACE'] == 'WHITE HISPANIC') |\n           (crimes['SUSP_RACE'] == 'WHITE') |\n           (crimes['SUSP_RACE'] == 'BLACK HISPANIC') |\n           (crimes['SUSP_RACE'] == 'ASIAN/PAC.ISL') |\n           (crimes['SUSP_RACE'] == 'AMER IND'), 'suspector_race_rand'] = crimes['SUSP_RACE']\n\n\n# Create copy to calculate proportions.\nsuspector_race_rand_copy = crimes['suspector_race_rand'].copy(deep = True)\n\n# Fill NaN values.\ncrimes['suspector_race_rand'].fillna(value = 1, inplace = True)\n\n# Obtain values for every race.\nsuspector_race_rand_copy.dropna(axis = 0, inplace = True)\nsorted_suspector_race_rand = suspector_race_rand_copy.value_counts(normalize = True).sort_index()\n\n# Fill one values in suspector_race with randomly picked from random choice.\ncrimes['suspector_race_rand'] = crimes['suspector_race_rand'].apply(lambda x: np.random.choice([x for x in sorted_suspector_race_rand.index],\n                                replace = True, p = sorted_suspector_race_rand) if (x == 1) else x).astype(str)\nprint(\"\\nFilled NaNs normalized:\\n\", crimes['suspector_race_rand'].value_counts(normalize = True))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c9a60e2e7f00c4bed8016e920accac9ef42b3956"
      },
      "cell_type": "code",
      "source": "# Similar operations for suspector sex.\n# I type similar code 3rd type...\nprint(\"Original data:\\n\", crimes['SUSP_SEX'].value_counts(dropna = False))\n\n# Create column for new variable suspector_age.\ncrimes['suspector_sex_rand'] = pd.Series(len(crimes['SUSP_SEX']), index = crimes.index)\ncrimes['suspector_sex_rand'] = 0\n\n# Randomly stick sex to every user with NaN value.\ncrimes.loc[(crimes['SUSP_SEX'] != 'M') | \n           (crimes['SUSP_SEX'] != 'F') |\n           (crimes['SUSP_SEX'].isnull() == True), 'suspector_sex_rand'] = np.nan\ncrimes.loc[(crimes['SUSP_SEX'] == 'M') | \n           (crimes['SUSP_SEX'] == 'F'), 'suspector_sex_rand'] = crimes['SUSP_SEX']\n\n\n# Create a copy to calculate proportions.\nsuspector_sex_rand_copy = crimes['suspector_sex_rand'].copy(deep = True)\n\n# Fill NaN values.\ncrimes['suspector_sex_rand'].fillna(value = 1, inplace = True)\n\n# Obtain values for every sex.\nsuspector_sex_rand_copy.dropna(axis = 0, inplace = True)\nsorted_suspector_sex_rand = suspector_sex_rand_copy.value_counts(normalize = True).sort_index()\n\n# Fill one values in suspector_sex_rand with randomly picked from random choice.\ncrimes['suspector_sex_rand'] = crimes['suspector_sex_rand'].apply(lambda x: np.random.choice([x for x in sorted_suspector_sex_rand.index],\n                                replace = True, p = sorted_suspector_sex_rand) if (x == 1) else x).astype(str)\nprint(\"Gender proportions after filled NaNs: \\n\", crimes['suspector_sex_rand'].value_counts(normalize = True))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "30de72bef07404491f9202ca076482f582696047"
      },
      "cell_type": "markdown",
      "source": "<br> Columns ***VIC_SEX***, ***VIC_AGE_GROUP*** and ***VIC_RACE*** have inappropriate values, but I leave them now and will fill using ML in near future. It could be easily filled with random function as above. You can try."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "45ce64749d231bf0b22ccde9e5b84b1bdb825fbb"
      },
      "cell_type": "code",
      "source": "# Informations about values of victims.\nprint(\"Age values: \", crimes['VIC_AGE_GROUP'].unique())\nprint(\"Sex values: \", crimes['VIC_SEX'].unique())\nprint(\" Race values: \", crimes['VIC_RACE'].unique())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "aef053046b866caa54312d2438cb9ea0c0e3b6a8"
      },
      "cell_type": "markdown",
      "source": "<a id=\"6\"></a> <br>\n### WHICH BOROUGH?"
    },
    {
      "metadata": {
        "_uuid": "270d4c18f726077c28a035ab3ea9f3b67f95c8ad"
      },
      "cell_type": "markdown",
      "source": "It's time to fill NaN values in BORO_NM column. I think there is one easy, but work demanding way to fill them. NY Boroughs boundaries data will be needed. It could be obtained from NYC Open Data (source: [CLICK](http://data.cityofnewyork.us/City-Government/Borough-Boundaries/tqmj-j8zm)) or directly from geopandas package which seems easier solution. In this kernel data come from NYC Open Data. "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "84eec238022299c09e6db1358637d25f7baa57c5"
      },
      "cell_type": "code",
      "source": "# Values of BORO_NM colum.\ncrimes['BORO_NM'].value_counts(dropna = False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7bcef2fa4c494f77695ff73340f7a72467efd6ef",
        "scrolled": true
      },
      "cell_type": "code",
      "source": "# Find rows with NaN values in crimes data frame.\nnan_boros = crimes[crimes['BORO_NM'].isnull()]\n# nan_boros.info()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "eed0bb8b172be4a1eeef0b572ea2761bf1c38117"
      },
      "cell_type": "markdown",
      "source": "The idea of finding BORO_NM value is simple. Longitude and latitude values of incidents from crimes data frame indicates if point of interest lies within boundries of one from 5 different boroughs. to check it it will be used *shapely* package."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "78a104c8fd31dcd0592626a093af8402df020b11"
      },
      "cell_type": "code",
      "source": "# Import NYC Borough boundries shape file. It is important to read in file using geopandas instead of pandas.\nboros = gpd.read_file('../input/nycboros/geo_export_366953f5-d29c-43ab-9e8e-781c703ad083.shp')\nboros['boro_name'] = boros['boro_name'].str.upper()\nboros",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "aa8b781bba55456c5d5b6c72019ab7e6cc7158ce",
        "scrolled": true
      },
      "cell_type": "code",
      "source": "# Function to make points - credits for https://twitter.com/dangerscarf\ndef make_point(row):\n    return Point(row.Longitude, row.Latitude)\n\n# Go through every row, and make a point out of its lon and lat\npoints = crimes.apply(make_point, axis=1)\n\n# New GeoDataFrame with old data and new geometry\ncrimes_geo = gpd.GeoDataFrame(crimes, geometry = points)\n\n# It doesn't come with a CRS because it's a CSV, so let's change it. I didn`t dive deep into it,\n# but I have read, that it`s needed to get right results.\ncrimes_geo.crs = {'init': 'epsg:4326'}\n\n# crimes_geo.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "23d4912aa6c09d773b4021ec6bbefacae34a64bc"
      },
      "cell_type": "code",
      "source": "# Fill NaN values in BORO_NM\ndef fill_boro_nan(gdf1, gdf2):\n    \"\"\" \n    Function fills NaN values in BORO_NM column in geodataframe and returns gdf with filled values.\n    gdf1 - geodataframe with NaN values and POINTS\n    gdf2 - geodataframe with POLYGONS and borougs names\n    \"\"\"\n    boro_list = [] # List for keeping values of BORO_NM\n    for point in gdf1['geometry'][gdf1['BORO_NM'].isnull()]:   # Iterate through rows with NaN values in BORO_NM column\n        for i in range(0, len(gdf2['geometry'])):   # Iterate through rows of boros data frame\n            if point.within(gdf2['geometry'][i]):   # Check if incident is within boundaries of one of boroughs\n                gdf1['BORO_NM'][gdf1['BORO_NM'].isnull()][i] = gdf2['boro_name'][i]   # Ascribe borough name to incident location\n                boro_list.append(gdf2['boro_name'][i])   # Make a list of boroughs\n            \n    boro_s = pd.Series(v for v in boro_list)    # Change list to series\n    # print(boro_s)\n    gdf1['BORO_NM'][gdf1['BORO_NM'].isnull()] = boro_s.values    # Fill NaN values in BORO_NM\n    return boro_s    # Will be usefull to check if fills are correct",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b2a5cf4f02fa79124bacc306bf43fe1970e3ecf2"
      },
      "cell_type": "code",
      "source": "#Execute\nboro_s = fill_boro_nan(crimes_geo, boros)\nnan_boros['BORO_NM'] = boro_s.values",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a1b1d4d2b655222a30bda5339cfb1055320cbb5a"
      },
      "cell_type": "code",
      "source": "# Check values\ncrimes_geo['BORO_NM'].value_counts(dropna = False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b29ed318dfadb4f884c2e354581da5e1e19b8f14"
      },
      "cell_type": "code",
      "source": "# Create geodataframe to check BORO_NM\nincidents = nan_boros[['BORO_NM', 'Latitude', 'Longitude']]\nincidents['Coordinates'] = list(zip(incidents['Longitude'], incidents['Latitude']))\nincidents['Coordinates'] = incidents['Coordinates'].apply(Point)\ngeo_incidents = gpd.GeoDataFrame(incidents, geometry = 'Coordinates')\n#print(geo_incidents.head())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "fdfdc6f200a78b88c512f8144ac3489fb5155558"
      },
      "cell_type": "code",
      "source": "# Check if filled values of BORO_NM corresponds with incident locations.\nbase = boros.plot(figsize = (16, 12), edgecolor = 'k', column = 'boro_name', cmap = 'Pastel1', alpha = 0.5)\ngeo_incidents.plot(ax = base, marker = 'o', column = 'BORO_NM', cmap = 'brg', markersize = 25, alpha = 0.8);\n# Everyhing seems to be OK!",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a57662cdeea6a212e5996ac9cc87b93defb2b137"
      },
      "cell_type": "code",
      "source": "# Drop useless values and save data frame for future use.\ncrimes_df = crimes_geo.drop(['CMPLNT_NUM', 'CRM_ATPT_CPTD_CD', 'KY_CD', 'OFNS_DESC', 'PREM_TYP_DESC', 'geometry', 'Lat_Lon', 'X_COORD_CD', 'Y_COORD_CD', 'RPT_DT'], axis = 1)\ncrimes_df.to_csv('crimes_df.csv', index = False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "11cc5bc4f9e70a019ee424320060fa7b50710fcc"
      },
      "cell_type": "code",
      "source": "# Check main data frame once again\ncrimes_df.info()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "5e050e902e2d4291cb682d9e81cc30382b3bceff"
      },
      "cell_type": "markdown",
      "source": "<a id=\"7\"></a> <br>\n### FINAL TOUGHTS"
    },
    {
      "metadata": {
        "_uuid": "a7218eb3b287d77accad0001a70aa851a34712a7"
      },
      "cell_type": "markdown",
      "source": "That's all in this notebook. In a few days I'm going to do the following:\n1. Write function to fill NaNs in suspectors' columns instead of typing similar code many times like above. **Update 2018-20-08**. *I wrote that function based on for loop, but it requires a lot of cumputational power, so I decided to drop idea o using it. I'll use it if I find different solution.*\n2. Fill NaNs in column BORO_PATROL with some clustering algorithm.**Update 2018-24-08** *I'll filled it in second kernel - [here](https://www.kaggle.com/mihalw28/nyc-crimes-2018-random-forest-regressor-nans).*\n3. Fill all the inapprioriate values in victims' columns using machine learning.\n4. Make static and interactive visualisations.\n\nThank you for your time."
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}